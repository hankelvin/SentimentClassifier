{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis of Twitter Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the required Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import string\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier \n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "# import spacy\n",
    "import timeit, re \n",
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the pickled train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load pickle file \n",
    "def pickleloader(filename):\n",
    "    # # open the file for writing\n",
    "    fileObject = open(filename,'rb') \n",
    "\n",
    "    # load the object from the file into var univ_processed_train\n",
    "    return pickle.load(fileObject,  encoding=\"latin1\")  #latin1 here, to bypass \n",
    "                                        # python2 to 3 pickle problem\n",
    "\n",
    "    # here we close the fileObject\n",
    "    fileObject.close()\n",
    "\n",
    "# function to create a file and store the data in the file \n",
    "def picklemaker(filename, objectname): \n",
    "    # open the file for writing\n",
    "    fileObject = open(filename,'wb') \n",
    "\n",
    "    # this writes the object a to the\n",
    "    # file named 'testfile'\n",
    "    pickle.dump(objectname,fileObject)   \n",
    "\n",
    "    # here we close the fileObject\n",
    "    fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the pickle file names as variables\n",
    "file_Name_train = \"tweet_train_complete_textnonull_PICKLE\"\n",
    "file_Name_test = \"tweet_test_complete_textnonull_PICKLE\"\n",
    "\n",
    "# run pickleloader and store results to variable\n",
    "train_complete = pickleloader(file_Name_train)\n",
    "test_complete = pickleloader(file_Name_test)\n",
    "\n",
    "#join the train and test sets together \n",
    "traintest_complete = pd.concat([train_complete,test_complete])\n",
    "\n",
    "# extracted only the RELATED rows \n",
    "traintest_complete_REL = traintest_complete[traintest_complete.filtering==\"RELATED\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the dataset for errors or unexpected inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEUTRAL', 'POSITIVE', 'NEGATIVE', 'RELATED'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check our target column\n",
    "traintest_complete_REL.polarity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check our predictors column (i.e. the tweet text)\n",
    "traintest_complete_REL.text.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target column is \"polarity\". Each row is only supposed to have either a \"POSITIVE\", NEUTRAL\" or \"NEGATIVE\" label. One or more rows has \"RELATED\" in it. The next steps are to locate them and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author</th>\n",
       "      <th>entity_id</th>\n",
       "      <th>text</th>\n",
       "      <th>filtering</th>\n",
       "      <th>polarity</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40361</th>\n",
       "      <td>216287162696605696</td>\n",
       "      <td>katita048</td>\n",
       "      <td>RL2013D04E169</td>\n",
       "      <td>Wisin &amp; Yandel - Follow The Leader ft. Jennife...</td>\n",
       "      <td>RELATED</td>\n",
       "      <td>RELATED</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40362</th>\n",
       "      <td>218086720443400192</td>\n",
       "      <td>isamarfernande</td>\n",
       "      <td>RL2013D04E169</td>\n",
       "      <td>Wisin &amp; Yandel - Follow The Leader ft. Jennife...</td>\n",
       "      <td>RELATED</td>\n",
       "      <td>RELATED</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id          author      entity_id  \\\n",
       "40361  216287162696605696       katita048  RL2013D04E169   \n",
       "40362  218086720443400192  isamarfernande  RL2013D04E169   \n",
       "\n",
       "                                                    text filtering polarity  \\\n",
       "40361  Wisin & Yandel - Follow The Leader ft. Jennife...   RELATED  RELATED   \n",
       "40362  Wisin & Yandel - Follow The Leader ft. Jennife...   RELATED  RELATED   \n",
       "\n",
       "         topic topic_priority  \n",
       "40361  neutral        NEUTRAL  \n",
       "40362  neutral        NEUTRAL  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traintest_complete_REL[traintest_complete_REL.polarity == \"RELATED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author</th>\n",
       "      <th>entity_id</th>\n",
       "      <th>text</th>\n",
       "      <th>filtering</th>\n",
       "      <th>polarity</th>\n",
       "      <th>topic</th>\n",
       "      <th>topic_priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tweet_id, author, entity_id, text, filtering, polarity, topic, topic_priority]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traintest_complete_REL.drop([40361, 40362], inplace=True)\n",
    "traintest_complete_REL[traintest_complete_REL.polarity == \"RELATED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = traintest_complete_REL.reset_index()\n",
    "dataset.drop([\"index\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data cleaning and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contain tweets that are English and Spanish. For the purpose of this project, we only want to use the English tweets. Therefore I have to find a way to easily filter them out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that determines whether a sentence is English or Spanish\n",
    "def is_it_this_language(textsource, tokeniser, languages):\n",
    "    # textsource, lDF, variable name of the the dataframe holding the sentence/document to be tokensized. if source is \n",
    "    # in a list, pass the list through pd.Dataframe() beforehand\n",
    "    # tokeniser, function name, this is the tokeniser to be used... this function is written to work with NLTK's word_tokenize, wordpunct_tokenize\n",
    "    # language, list, the language(s) to be checked for. NLTK's language are lowercase, full words e.g. \"english\". \n",
    "    \n",
    "    it_is_this_language = {}\n",
    "    for i in list(textsource.index):\n",
    "        tokenised = tokeniser(textsource[i])\n",
    "        tokens_lowercase = set([token.lower() for token in tokenised]) # using sets here so we can use set intersection below\n",
    "    \n",
    "        languages_ratios = {}\n",
    "        for lang in languages: \n",
    "            stopwords_set = set(stopwords.words(lang))\n",
    "            common_elements = tokens_lowercase.intersection(stopwords_set)\n",
    "            languages_ratios[lang] = len(common_elements) #appends the number of tokens in the text that falls \n",
    "                                                              #in each of the NLTK's stop words.   \n",
    "        it_is_this_language[i] = max(languages_ratios, key=languages_ratios.get)\n",
    "    return it_is_this_language\n",
    "\n",
    "# run the function with a timer. languages to be used are English and Spanish. This is due to the manner in which the \n",
    "# dataset was collected. \n",
    "languages_input = [\"english\", \"spanish\"]\n",
    "language = is_it_this_language(dataset.text, wordpunct_tokenize,languages_input)\n",
    "\n",
    "dataset[\"tweet_lang\"] = language.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105003, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are only keeping the English tweets for this analysis. \n",
    "dataset_eng = dataset[dataset.tweet_lang==\"english\"].copy()\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-processing of the English tweets. Using singular tokens to represent sentiment emoticons, capitalisation etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write functions that uses regex to preprocess the tweet texts in the same manner as the Stanford GloVe Twitter set\n",
    "# https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb | neutralface and ALLCAPS are left out as they are \n",
    "# ambiguous \n",
    "# apply all these function first (such that # and @ can be used to locate these features. \n",
    "# followed by a final removal of special characters.  \n",
    "\n",
    "def replaceurls(tweettext):\n",
    "    return re.sub(r\"http\\B\\S+|www\\.\\B\\S+|\\S+\\.\\w+\", ' <URL> ', tweettext)\n",
    "\n",
    "def replacementions(tweettext):\n",
    "    return re.sub(r\"(@|@\\B)\\w+\", ' <USER> ', tweettext) \n",
    "\n",
    "def replacenumbers(tweettext):\n",
    "    return re.sub(r'\\s(\\+|-)*\\d+\\b', \" <NUMBER> \", tweettext) # slight difference from the GloVe treatment which also replaces +- \n",
    "                                                      # hypothesis is that +- numbers do not serve as signal for sentiment \n",
    "                                                        # only change numbers that start and end with whitespace. we don't\n",
    "                                                        # want to replace instances such as test123 \n",
    "\n",
    "def replacehashtags(tweettext):\n",
    "    return re.sub(r'(#|#\\B)\\w+', \" <HASHTAG> \", tweettext)\n",
    "\n",
    "def replacerepeatexclaim(tweettext):\n",
    "    return re.sub(r'(([!.]){2,})', '! <REPEAT> ', tweettext) # replace multiple !!! with ! and <REPEAT>. i.e. new feature\n",
    "\n",
    "def replacerepeatquestion(tweettext):\n",
    "    return re.sub(r'(([?.]){2,})', '? <REPEAT> ', tweettext) # replace multiple ??? with ? and <REPEAT>. i.e. new feature\n",
    "\n",
    "def replacesmiles(tweettext):\n",
    "    return re.sub(r\"(8|:|=|;)('|`|-)?\\){1,2}\", \" <SMILE> \", tweettext)\n",
    "\n",
    "def replacelolface(tweettext):\n",
    "    return re.sub(r\"(8|:|=|;)('|`|-)?(P|p|D){1,3}\", \" <LOLFACE> \", tweettext) # slight difference from the GloVe\n",
    "                                                                                    # treatment... catches :p and :d \n",
    "def replacesadface(tweettext):\n",
    "    return re.sub(r\"(8|:|=|;)('|`|-)?\\({1,2}\", \" <SADFACE> \", tweettext)\n",
    "\n",
    "def replaceheart(tweettext):\n",
    "    return re.sub(r\"<3{1,}\", \" <HEART> \", tweettext)\n",
    "\n",
    "def replaceelongword(tweettext):\n",
    "    return re.sub(r'(\\S*?)\\B(.)\\2{2,}\\b', r'\\1' + r'\\2' + ' <ELONG> ', tweettext)\n",
    "\n",
    "def replaceallcaps(tweettext):\n",
    "    return re.sub(r'(([A-Z]){7,}\\B)', ' <ALLCAPS> ' + r'\\1', tweettext) # regex for tokens composed of a sequence of \n",
    "                                                                        # 7 or more capitalised alphabets. 7 because it \n",
    "                                                                        # is unlikely to be a acronym (e.g. UNICEF)\n",
    "        \n",
    "tweetprocessorfuncs = [replaceurls, replacementions, replacenumbers, replacehashtags, replacerepeatexclaim, \\\n",
    "                       replacerepeatquestion, replacesmiles, replacelolface, replacesadface, replaceheart,\\\n",
    "                       replaceelongword, replaceallcaps]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the Pre-processing function across all English tweets\n",
    "for i in tweetprocessorfuncs:\n",
    "    dataset_eng.text = dataset_eng.text.apply(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that uses regex to remove remaining @ # and other special characters as well as english stop words. \n",
    "\n",
    "specialchars = re.compile('[^0-9a-z ?!<>+_]') # the GloVe Twitter set left ! and ? in place. we replicate the same \n",
    "                                              # to preserve any associated semantic pattern  \n",
    "stopwords_list = set(stopwords.words('english')) #using NLTK's english stopwords\n",
    "\n",
    "entity_names =[\"bmw\", \"audi\", \"volvo\", \"toyota\", \"volkswagen\", \"honda\", \"nissan\", \"fiat\", \"suzuki\", \"porsche\",\n",
    "              \"yamaha\", \"mazda\", \"chrysler\", \"subaru\", \"ferrari\", \"bentley\", \"kia\", \"ford\", \"jaguar\", \"lexus\",\n",
    "              \"barclays\", \"fargo\", \"wellsfargo\", \"bankia\", \"santander\", \"goldman\", \"harvard\", \"standford\", \n",
    "              \"berkeley\", \"princeton\", \"columbia\", \"yale\", \"hopkins\", \"johnhopkins\", \"oxford\", \"adele\", \"aliciakeys\", \n",
    "               \"alicia\",\"beatles\", \"zeppelin\", \"aerosmith\", \"bonjovi\", \"jovi\", \"acdc\", \"thewanted\", \"coldplay\", \n",
    "               \"ladygaga\",\"gaga\", \"madonna\", \"jennifer\", \"lopez\", \"jeniferlopez\", \"bieber\", \"justinbieber\", \"shakira\", \"psy\",\n",
    "              \"whitneyhouston\", \"whitney\", \"houston\", \"britneyspears\", \"britney\", \"spears\"]\n",
    "\n",
    "for i in entity_names: \n",
    "    stopwords_list.add(i)\n",
    "\n",
    "def finaltweetclean(tweettext):\n",
    "    tweettext = tweettext.lower() # lowercase text\n",
    "    tweettext = re.sub(\"  \", \" \", tweettext) # delete double spaces\n",
    "    tweettext = re.sub(\"  \", \" \", tweettext) # delete double spaces\n",
    "    tweettext = re.sub(\"  \", \" \", tweettext) # delete double spaces\n",
    "    tweettext = re.sub(\"  \", \" \", tweettext) # delete double spaces\n",
    "    tweettext = re.sub(specialchars, \"\", tweettext) # delete symbols which are in specialchars from text\n",
    "    tweettext = ' '.join([i for i in tweettext.split() if i not in stopwords_list]) # delete stopwords from text\n",
    "    return tweettext\n",
    "\n",
    "dataset_eng.text = dataset_eng.text.apply(finaltweetclean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use gensim to load the GloVe Twitter trained vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec # for calling glove2word2vec script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1193514"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the GloVe format is not aligned for use with gensim. The following gensim script automatically converts it and \n",
    "# loads it. \n",
    "\n",
    "glove_file = ('./glove.twitter.27B.100d.txt')\n",
    "tmp_file = get_tmpfile(\"glove_word2vec.txt\")\n",
    "# default way (through CLI): python -m gensim.scripts.glove2word2vec --input <glove_file> --output <w2v_file>\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "glove_twitter = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "len(glove_twitter.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty nested dictionary to store the word vectors based on the following classification: \n",
    "# (i) first character being alphabet, (ii) being a special token, (iii) first letter being a punctuation, except \n",
    "# special token, (iv) all others. \n",
    "\n",
    "glove_twitter_vocab = {\"spectok\":{},\"specchar\":{}}\n",
    "for alphabet in [alpha for alpha in string.ascii_lowercase]:\n",
    "    glove_twitter_vocab[alphabet] = {}\n",
    "for punctuation in [punct for punct in string.punctuation]:\n",
    "    glove_twitter_vocab[punctuation] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the GloVe Twitter model \n",
    "for i in glove_twitter.vocab: \n",
    "    if i[0] in [alpha for alpha in string.ascii_lowercase]:\n",
    "        glove_twitter_vocab[i[0]].update({i:glove_twitter.get_vector(i)})\n",
    "    elif i in [\"<user>\", \"<url>\", \"<number>\", \"<hashtag>\", \"<repeat>\", \"<smile>\", \"<lolface>\", \\\n",
    "             \"<sadface>\", \"<heart>\", \"<elong>\", \"<allcaps>\"]:\n",
    "        glove_twitter_vocab[\"spectok\"].update({i:glove_twitter.get_vector(i)})\n",
    "    elif i[0] in [punct for punct in string.punctuation]:\n",
    "        glove_twitter_vocab[i[0]].update({i:glove_twitter.get_vector(i)})\n",
    "    else: \n",
    "        glove_twitter_vocab[\"specchar\"].update({i:glove_twitter.get_vector(i)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that (i) checks whether each of the token in a single tweet is in the Twitter-trained 100D GloVe set.\n",
    "# (ii) if not, input a matrix of the same embedding size - (100,1) in this case -\n",
    "spacynlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def paravectorPOS(text): \n",
    "    word_list = []\n",
    "    place_token = np.zeros_like(glove_twitter.get_vector(\"the\"))+0.01\n",
    "    if True in [i2 in glove_twitter.vocab for i2 in [i for i in text.split()]]:\n",
    "        spacy_adjadv_list = [token.text for token in spacynlp(text) if token.pos_ in [\"ADJ\",\"ADV\"]]\n",
    "\n",
    "        for i3 in text.split(): \n",
    "            if i3[0] in [alpha for alpha in string.ascii_lowercase]: \n",
    "                if i3 in glove_twitter_vocab[i3[0]].keys():\n",
    "                    if i3 in spacy_adjadv_list:\n",
    "                        word_list.append(glove_twitter.get_vector(i3)**2)  # we square the adjectives and adverbs\n",
    "                                                                           # to give them a higher/lower weight \n",
    "                                                                           # with respective to positive/negative \n",
    "                                                                           # sentiments \n",
    "                    else: \n",
    "                        word_list.append(glove_twitter.get_vector(i3))\n",
    "            \n",
    "            \n",
    "            elif i3 in glove_twitter_vocab[\"spectok\"].keys():\n",
    "                word_list.append(glove_twitter.get_vector(i3))  \n",
    "            \n",
    "            \n",
    "            elif i3[0] in [punct for punct in string.punctuation]: \n",
    "                if i3 in glove_twitter_vocab[i3[0]].keys():\n",
    "                    if i3 in spacy_adjadv_list:\n",
    "                        word_list.append(glove_twitter.get_vector(i3)**2) # we square the adjectives and adverbs\n",
    "                                                                           # to give them a higher/lower weight \n",
    "                                                                           # with respective to positive/negative \n",
    "                                                                           # sentiments \n",
    "                    else: \n",
    "                        word_list.append(glove_twitter.get_vector(i3))\n",
    "            \n",
    "            else: \n",
    "                word_list.append(place_token)\n",
    "    \n",
    "    else:\n",
    "        word_list.append(place_token) \n",
    "    return sum(word_list)/len(word_list)\n",
    "\n",
    "# for tweets with no tokens in the GloVe Twitter embeddings, we impute a similar sized vector of 0.01s. a check of \n",
    "# shows that the most similar words to this vectors are infrequent words and do not hold significant meaning for sentiment. \n",
    "# glove_twitter.similar_by_vector(np.zeros_like(glove_twitter.get_vector(\"the\")+0.01))\n",
    "# [('bitmeye', 0.0),\n",
    "#  ('bispa', 0.0),\n",
    "#  ('bosanma', 0.0),\n",
    "#  ('bord√µes', 0.0),\n",
    "#  ('bookmarklet', 0.0),\n",
    "#  ('boncabe', 0.0),\n",
    "#  ('bocs', 0.0),\n",
    "#  ('bloked', 0.0),\n",
    "#  ('boyet', 0.0),\n",
    "#  ('bilmeyi', 0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the paravectorPOS function (i.e. taking into consideration PoS and giving higher weights to adj and adverbs)\n",
    "\n",
    "allparavectors = [] #empty list to append each tweets DF (containing summed GloVe)\n",
    "for k in dataset_eng.text:  # \n",
    "    allparavectors.append(pd.DataFrame(paravectorPOS(k)).T) \n",
    "    \n",
    "X_SATask_eng_GloVe = pd.concat(allparavectors,axis=0) #concat the GloVe-ed dataframes for each tweet\n",
    "X_SATask_eng_GloVe.index = dataset_eng.index #restore the SATask_eng index to the new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picklemaker(\"X_SATask_eng_GloVe_PICKLED\",X_SATask_eng_GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SATask_eng_GloVe = pickleloader(\"X_SATask_eng_GloVe_PICKLED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_SATask_eng_GloVe, \n",
    "                                                    dataset_eng.polarity, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 18422,   6442,  19577,   4040,  97325,  19764,  60200,   3979,\n",
       "             58172,  62327,\n",
       "            ...\n",
       "             19526,  74102,  53398,  45212, 102138,   7311,  68181,  94459,\n",
       "              1019,  19294],\n",
       "           dtype='int64', length=56980)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MMScale = MinMaxScaler()\n",
    "X_train = MMScale.fit_transform(X_train)\n",
    "X_test = MMScale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBScaler = RobustScaler()\n",
    "# X_train = RBScaler.fit_transform(X_train)\n",
    "# X_test = RBScaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The count for the \"NEGATIVE\" class is significantly lower than \"POSITIVE\". Use SMOTE to upsample the \"NEGATIVE\" class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/imblearn/utils/deprecation.py:50: DeprecationWarning: 'k' is deprecated from 0.2 and will be removed in 0.4. Use 'k_neighbors' instead.\n",
      "  category=DeprecationWarning)\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/imblearn/utils/deprecation.py:50: DeprecationWarning: 'm' is deprecated from 0.2 and will be removed in 0.4. Use 'm_neighbors' instead.\n",
      "  category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(82985, 100)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = SMOTETomek(ratio=\"minority\", random_state=42, \n",
    "                smote=SMOTE(ratio=\"minority\",random_state=42, k_neighbors=50,m_neighbors=50, n_jobs=-1),\\\n",
    "                tomek=TomekLinks(ratio=\"not minority\",n_jobs=-1), n_jobs=-1) \n",
    "# , enn=EditedNearestNeighbours(n_neighbors=5, n_jobs=-1)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "X_train_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83671, 100)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from imblearn.over_sampling import ADASYN\n",
    "# adasyn = ADASYN(ratio=\"minority\", random_state=42, n_neighbors=5, n_jobs=-1) \n",
    "\n",
    "# X_train_res, y_train_res = adasyn.fit_sample(X_train, y_train)\n",
    "# X_train_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result of best-tuned Random Forest Classifier - with POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=80,random_state=42, oob_score=True, n_jobs=-1,class_weight=\"balanced_subsample\")\n",
    "clf_model = clf.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_cvs_train = cross_val_score(clf_model, X_train_res, y=y_train_res)\n",
    "RFC_cvs_test = cross_val_score(clf_model, X_test, y=y_test)\n",
    "RFC_cr = classification_report(y_test, clf_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFClass 100 estimators, class_weight=balanced_subsample | using MinMaxScaler | using SMOTETomek k = 50, m = 50 \n",
      "\n",
      " train cross val score:  \n",
      " [0.77127467 0.78573494 0.78699252] \n",
      "\n",
      " test cross val score:  \n",
      " [0.64724746 0.65141635 0.6525922 ] \n",
      "\n",
      " class report - test set:  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   NEGATIVE       0.44      0.54      0.49      3333\n",
      "    NEUTRAL       0.73      0.20      0.32      7893\n",
      "   POSITIVE       0.69      0.90      0.78     16839\n",
      "\n",
      "avg / total       0.67      0.66      0.62     28065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RFClass 100 estimators, class_weight=balanced_subsample | using MinMaxScaler | using SMOTETomek k = 50, m = 50\", \"\\n\\n\", \\\n",
    "           \"train cross val score: \", \"\\n\", RFC_cvs_train, \"\\n\\n\",\n",
    "           \"test cross val score: \", \"\\n\", RFC_cvs_test, \"\\n\\n\",\n",
    "           \"class report - test set: \", \"\\n\", RFC_cr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFClass 100 estimators, class_weight=balanced | using MinMaxScaler | using SMOTETomek k = 50, m = 50 \n",
      "\n",
      " train cross val score:  \n",
      " [0.77105777 0.7862772  0.78648639] \n",
      "\n",
      " test cross val score:  \n",
      " [0.6484233  0.65098878 0.65366114] \n",
      "\n",
      " class report - test set:  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   NEGATIVE       0.44      0.54      0.49      3333\n",
      "    NEUTRAL       0.72      0.20      0.32      7893\n",
      "   POSITIVE       0.69      0.89      0.78     16839\n",
      "\n",
      "avg / total       0.67      0.66      0.62     28065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RFClass 100 estimators, class_weight=balanced | using MinMaxScaler | using SMOTETomek k = 50, m = 50\", \"\\n\\n\", \\\n",
    "           \"train cross val score: \", \"\\n\", RFC_cvs_train, \"\\n\\n\",\n",
    "           \"test cross val score: \", \"\\n\", RFC_cvs_test, \"\\n\\n\",\n",
    "           \"class report - test set: \", \"\\n\", RFC_cr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFClass 100 estimators | using MinMaxScaler | using SMOTETomek k = 50, m = 50 \n",
      "\n",
      " train cross val score:  \n",
      " [0.77138312 0.78945846 0.78775171] \n",
      "\n",
      " test cross val score:  \n",
      " [0.65408872 0.65195083 0.65611972] \n",
      "\n",
      " class report - test set:  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   NEGATIVE       0.45      0.54      0.49      3333\n",
      "    NEUTRAL       0.72      0.21      0.33      7893\n",
      "   POSITIVE       0.70      0.90      0.78     16839\n",
      "\n",
      "avg / total       0.67      0.66      0.62     28065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RFClass 100 estimators | using MinMaxScaler | using SMOTETomek k = 50, m = 50\", \"\\n\\n\", \\\n",
    "           \"train cross val score: \", \"\\n\", RFC_cvs_train, \"\\n\\n\",\n",
    "           \"test cross val score: \", \"\\n\", RFC_cvs_test, \"\\n\\n\",\n",
    "           \"class report - test set: \", \"\\n\", RFC_cr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFClass 100 estimators | using MinMaxScaler | using SMOTETomek k = 10, m = 10 \n",
      "\n",
      " train cross val score:  \n",
      " [0.77138312 0.78945846 0.78775171] \n",
      "\n",
      " test cross val score:  \n",
      " [0.65408872 0.65195083 0.65611972] \n",
      "\n",
      " class report - test set:  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   NEGATIVE       0.45      0.54      0.49      3333\n",
      "    NEUTRAL       0.72      0.21      0.33      7893\n",
      "   POSITIVE       0.70      0.90      0.78     16839\n",
      "\n",
      "avg / total       0.67      0.66      0.62     28065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RFClass 100 estimators | using MinMaxScaler | using SMOTETomek k = 10, m = 10\", \"\\n\\n\", \\\n",
    "           \"train cross val score: \", \"\\n\", RFC_cvs_train, \"\\n\\n\",\n",
    "           \"test cross val score: \", \"\\n\", RFC_cvs_test, \"\\n\\n\",\n",
    "           \"class report - test set: \", \"\\n\", RFC_cr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFClass 100 estimators | using MinMaxScaler | using SMOTETomek k = 5, m = 5 \n",
      "\n",
      " train cross val score:  \n",
      " [0.77138312 0.78945846 0.78775171] \n",
      "\n",
      " test cross val score:  \n",
      " [0.65408872 0.65195083 0.65611972] \n",
      "\n",
      " class report - test set:  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   NEGATIVE       0.45      0.54      0.49      3333\n",
      "    NEUTRAL       0.72      0.21      0.33      7893\n",
      "   POSITIVE       0.70      0.90      0.78     16839\n",
      "\n",
      "avg / total       0.67      0.66      0.62     28065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RFClass 100 estimators | using MinMaxScaler | using SMOTETomek k = 5, m = 5\", \"\\n\\n\", \\\n",
    "           \"train cross val score: \", \"\\n\", RFC_cvs_train, \"\\n\\n\",\n",
    "           \"test cross val score: \", \"\\n\", RFC_cvs_test, \"\\n\\n\",\n",
    "           \"class report - test set: \", \"\\n\", RFC_cr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFClass 100 estimators | using MinMaxScaler | using ADASYN n = 5 \n",
      "\n",
      " train cross val score:  \n",
      " [0.73686852 0.76103403 0.75983363] \n",
      "\n",
      " test cross val score:  \n",
      " [0.65408872 0.65195083 0.65611972] \n",
      "\n",
      " class report - test set:  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   NEGATIVE       0.42      0.55      0.48      3333\n",
      "    NEUTRAL       0.72      0.21      0.32      7893\n",
      "   POSITIVE       0.70      0.89      0.78     16839\n",
      "\n",
      "avg / total       0.67      0.66      0.62     28065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RFClass 100 estimators | using MinMaxScaler | using ADASYN n = 5\", \"\\n\\n\", \\\n",
    "           \"train cross val score: \", \"\\n\", RFC_cvs_train, \"\\n\\n\",\n",
    "           \"test cross val score: \", \"\\n\", RFC_cvs_test, \"\\n\\n\",\n",
    "           \"class report - test set: \", \"\\n\", RFC_cr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFClass 100 estimators | using MinMaxScaler | using SMOTETomek, k=5, m=5 \n",
      "\n",
      " train cross val score:  \n",
      " [0.77138312 0.78945846 0.78775171] \n",
      "\n",
      " test cross val score:  \n",
      " [0.65408872 0.65195083 0.65611972] \n",
      "\n",
      " class report - test set:  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   NEGATIVE       0.45      0.54      0.49      3333\n",
      "    NEUTRAL       0.72      0.21      0.33      7893\n",
      "   POSITIVE       0.70      0.90      0.78     16839\n",
      "\n",
      "avg / total       0.67      0.66      0.62     28065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RFClass 100 estimators | using MinMaxScaler | using SMOTETomek, k=5, m=5\", \"\\n\\n\", \\\n",
    "           \"train cross val score: \", \"\\n\", RFC_cvs_train, \"\\n\\n\",\n",
    "           \"test cross val score: \", \"\\n\", RFC_cvs_test, \"\\n\\n\",\n",
    "           \"class report - test set: \", \"\\n\", RFC_cr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFClass 100 estimators | using MinMaxScaler | using ADASYN \n",
      "\n",
      " train cross val score:  \n",
      " [0.73098792 0.76350391 0.76160353] \n",
      "\n",
      " test cross val score:  \n",
      " [0.65408872 0.65195083 0.65611972] \n",
      "\n",
      " class report - test set:  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   NEGATIVE       0.42      0.55      0.48      3333\n",
      "    NEUTRAL       0.73      0.21      0.33      7893\n",
      "   POSITIVE       0.70      0.89      0.78     16839\n",
      "\n",
      "avg / total       0.68      0.66      0.62     28065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RFClass 100 estimators | using MinMaxScaler | using ADASYN\", \"\\n\\n\", \\\n",
    "           \"train cross val score: \", \"\\n\", RFC_cvs_train, \"\\n\\n\",\n",
    "           \"test cross val score: \", \"\\n\", RFC_cvs_test, \"\\n\\n\",\n",
    "           \"class report - test set: \", \"\\n\", RFC_cr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFClass 100 estimators | using MinMaxScaler \n",
      "\n",
      " train cross val score:  \n",
      " [0.77138312 0.78945846 0.78775171] \n",
      "\n",
      " test cross val score:  \n",
      " [0.65408872 0.65195083 0.65611972] \n",
      "\n",
      " class report - test set:  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   NEGATIVE       0.45      0.54      0.49      3333\n",
      "    NEUTRAL       0.72      0.21      0.33      7893\n",
      "   POSITIVE       0.70      0.90      0.78     16839\n",
      "\n",
      "avg / total       0.67      0.66      0.62     28065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RFClass 100 estimators | using MinMaxScaler\", \"\\n\\n\", \\\n",
    "           \"train cross val score: \", \"\\n\", RFC_cvs_train, \"\\n\\n\",\n",
    "           \"test cross val score: \", \"\\n\", RFC_cvs_test, \"\\n\\n\",\n",
    "           \"class report - test set: \", \"\\n\", RFC_cr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFClass 100 estimators \n",
      "\n",
      " train cross val score:  \n",
      " [0.77068729 0.79058254 0.7894908 ] \n",
      "\n",
      " test cross val score:  \n",
      " [0.64949225 0.65440941 0.65804383] \n",
      "\n",
      " class report - test set:  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   NEGATIVE       0.45      0.54      0.49      3333\n",
      "    NEUTRAL       0.72      0.21      0.32      7893\n",
      "   POSITIVE       0.69      0.90      0.78     16839\n",
      "\n",
      "avg / total       0.67      0.66      0.62     28065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RFClass 100 estimators | using RobustScaler\", \"\\n\\n\", \\\n",
    "           \"train cross val score: \", \"\\n\", RFC_cvs_train, \"\\n\\n\",\n",
    "           \"test cross val score: \", \"\\n\", RFC_cvs_test, \"\\n\\n\",\n",
    "           \"class report - test set: \", \"\\n\", RFC_cr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running a pipeline with GridSearch CV to find the best performing classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfClass \n",
      "\n",
      " train cross val score:  \n",
      " [0.77232304 0.78837394 0.78786016] \n",
      "\n",
      " test cross val score:  \n",
      " [0.77232304 0.78837394 0.78786016] \n",
      "\n",
      " class report - test set:  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   NEGATIVE       0.45      0.55      0.49      3333\n",
      "    NEUTRAL       0.74      0.20      0.32      7893\n",
      "   POSITIVE       0.69      0.90      0.78     16839\n",
      "\n",
      "avg / total       0.68      0.66      0.62     28065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "est_names = [\"rfClass\", \"sgdClass\", 'decisiontree']\n",
    "\n",
    "estimators = [RandomForestClassifier(random_state=42), \n",
    "              SGDClassifier(random_state=42, n_jobs=-1),DecisionTreeClassifier(random_state=42)] \n",
    "              \n",
    "parameters = [\n",
    "    {est_names[0]+'__n_estimators': np.arange(70, 101 ,10),\n",
    "    est_names[0]+'__oob_score': (True, False),\n",
    "    est_names[0]+'__class_weight':('balanced', 'balanced_subsample')},\n",
    "    {est_names[1]+'__loss': ('hinge', 'log', 'modified_huber'), \n",
    "     est_names[1]+'__penalty': ('l1','l2', 'elasticnet'),\n",
    "     est_names[1]+'__alpha': ([10 ** x for x in range(-6, -2)]),\n",
    "     est_names[1]+'__tol' : ([10 ** x for x in range(-4, -2)]),\n",
    "     est_names[1]+'__max_iter':(np.arange(300, 500, 50))},\n",
    "    {est_names[2]+'__criterion': (\"gini\", \"entropy\"), \n",
    "     est_names[2]+'__max_features' : (np.arange(0.5,1.01,0.25))}\n",
    "             ]\n",
    "\n",
    "# for RFCLass, we don't limit max depth... \n",
    "# for TFIDF normalisation read 2.2.3 of https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf\n",
    "# for TFIDF sublinear_tf = True is necessary because it provides smoothing. \n",
    "# for SparsePCA read https://gawalt.com/brian/resources/TextEDAviaSparsePCA.pdf\n",
    "\n",
    "gridsearch_models = []\n",
    "for est_name, estimator, param in zip(est_names, estimators, parameters):\n",
    "    pipe =  Pipeline([(est_name, estimator)])\n",
    "    pipe_GS = GridSearchCV(pipe,param_grid=param, n_jobs=-1, cv=3)\n",
    "    clf = pipe_GS.fit(X_train_res , y=y_train_res)\n",
    "    gridsearch_models.append(clf)\n",
    "    pipe_GS_cvs_train = cross_val_score(clf, X_train_res, y=y_train_res)\n",
    "    pipe_GS_cvs_test = cross_val_score(clf, X_test, y=y_test)\n",
    "    pipe_GS_cr = classification_report(y_test, clf.predict(X_test))                 \n",
    "    print(est_name, \"\\n\\n\", \\\n",
    "           \"train cross val score: \", \"\\n\", pipe_GS_cvs_train, \"\\n\\n\",\n",
    "           \"test cross val score: \", \"\\n\", pipe_GS_cvs_train, \"\\n\\n\",\n",
    "           \"class report - test set: \", \"\\n\", pipe_GS_cr)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(),\n",
    "    n_estimators=10,\n",
    "    learning_rate=1)\n",
    "\n",
    "adaboost_clf = adaboost.fit(X_train_res, y_train_res)\n",
    "adaboost_cvs_train = cross_val_score(adaboost_clf, X_train_res, y=y_train_res)\n",
    "adaboost_cvs_test = cross_val_score(adaboost_clf, X_test, y=y_test)\n",
    "adaboost_cr = classification_report(y_test, adaboost_clf.predict(X_test))                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost with DecisionTreeClassifier \n",
      "\n",
      " train cross val score:  \n",
      " [0.70200998 0.71545803 0.72307581] \n",
      "\n",
      " test cross val score:  \n",
      " [0.58770711 0.59091395 0.5797969 ] \n",
      "\n",
      " class report - test set:  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   NEGATIVE       0.33      0.50      0.40      3333\n",
      "    NEUTRAL       0.47      0.31      0.38      7893\n",
      "   POSITIVE       0.70      0.74      0.72     16839\n",
      "\n",
      "avg / total       0.59      0.59      0.59     28065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Adaboost with DecisionTreeClassifier\", \"\\n\\n\", \\\n",
    "           \"train cross val score: \", \"\\n\", adaboost_cvs_train, \"\\n\\n\",\n",
    "           \"test cross val score: \", \"\\n\", adaboost_cvs_test, \"\\n\\n\",\n",
    "           \"class report - test set: \", \"\\n\", adaboost_cr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost = AdaBoostClassifier(\n",
    "    RandomForestClassifier(n_estimators=80,random_state=42, oob_score=True, n_jobs=-1,\\\n",
    "                           class_weight=\"balanced_subsample\"),\n",
    "    n_estimators=10,\n",
    "    learning_rate=1)\n",
    "\n",
    "adaboost_clf = adaboost.fit(X_train_res, y_train_res)\n",
    "adaboost_cvs_train = cross_val_score(adaboost_clf, X_train_res, y=y_train_res)\n",
    "adaboost_cvs_test = cross_val_score(adaboost_clf, X_test, y=y_test)\n",
    "adaboost_cr = classification_report(y_test, adaboost_clf.predict(X_test))                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost with RandomForestClassifier \n",
      "\n",
      " train cross val score:  \n",
      " [0.77456438 0.78653026 0.7876071 ] \n",
      "\n",
      " test cross val score:  \n",
      " [0.63303046 0.63858899 0.63976483] \n",
      "\n",
      " class report - test set:  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   NEGATIVE       0.45      0.56      0.50      3333\n",
      "    NEUTRAL       0.80      0.15      0.26      7893\n",
      "   POSITIVE       0.69      0.91      0.78     16839\n",
      "\n",
      "avg / total       0.69      0.66      0.60     28065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Adaboost with RandomForestClassifier\", \"\\n\\n\", \\\n",
    "           \"train cross val score: \", \"\\n\", adaboost_cvs_train, \"\\n\\n\",\n",
    "           \"test cross val score: \", \"\\n\", adaboost_cvs_test, \"\\n\\n\",\n",
    "           \"class report - test set: \", \"\\n\", adaboost_cr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
